{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_ex_model_assessment",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXol0MrgrGLX"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/exercises/6_ex_model_assessment.ipynb) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Autq7nJ8pGJb"
      },
      "source": [
        "# Exercises for Model Assessment \n",
        "\n",
        "This notebook is based on the [Model Assessment Notebook](https://). \n",
        "This notebook will guide you through related tasks to strengthen your understanding of these concepts & Python programming. After completing this excercise you will be another step closer to becoming a data scientist!\n",
        "\n",
        "Before we start, we will undergo the following steps:\n",
        "- import standard libraries and set plotting parameters\n",
        "- import data and define target variable and features\n",
        "- splitting the data (if you are not familiar with this concept please return to the tutorial)\n",
        "- train a logit and tree model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ-ZuTe9sAmw"
      },
      "source": [
        "# Importing standard libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "\n",
        "# Set parameters for plotting\n",
        "%matplotlib inline  \n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTEPtjAZrj65"
      },
      "source": [
        "# Import data\n",
        "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq_modeling.csv' \n",
        "df = pd.read_csv(data_url, index_col=\"index\")\n",
        "\n",
        "# Split data into target and features\n",
        "X = df.drop(['BAD'], axis=1) \n",
        "y = df['BAD']\n",
        "\n",
        "# Zero-one encoding of the target\n",
        "df['BAD'] = df['BAD'].astype(int) "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOi4qEAAsZFX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=888)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lb9Ujwtlqay",
        "outputId": "d3575613-9c36-4558-ed43-4dcd3c5f23b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Estimate a logit model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logit = LogisticRegression(penalty='none', fit_intercept=True)\n",
        "logit.fit(X_train, y_train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='none',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OAzxQ9olqa9",
        "outputId": "1a72d3f8-be27-423b-85d0-64f78b0ed907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Estimate a CART tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
        "tree.fit(X_train, y_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=5, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JoWJntgtkHh"
      },
      "source": [
        "## Tasks \n",
        "\n",
        "In case you might have forgotten some general model assessment procedures, we will start with some simple tasks to get you back into the topic! \n",
        "\n",
        "1. Create distinct class predictions for the tree and logit model. Save the results for task 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCSL49mXvh9y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRd_EhY1t4tp"
      },
      "source": [
        "2. Create the corresponding confusion matrix and manually compute the accuracy, precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJdHSVDwuS3V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc9JN3I-uTDV"
      },
      "source": [
        "Are these results representative for how good the model is? What are the shortfalls of the accuracy measure?\n",
        "___\n",
        "\n",
        "3. Calculate the  class probabilities for both models for class 1. Save the results for task 4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I0Hdc7JvPno"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNG18JYZvQHl"
      },
      "source": [
        "4. Calculate the corresponding AUC values for both predictions and plot the ROC curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQFDkYSWvs5T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0XzftqIvtBf"
      },
      "source": [
        "How can we interpret this measure? Is it representative?\n",
        "____\n",
        "Let's have a closer look at how the cut-offs influence our final results. \n",
        "5. Calculate the class probabilities and use these to create distinct class predictions for multiple cut-offs. Vary the cut-off from 0 to 1 in step-sizes of 0.01. Save the accuracy results and corresponding cut-offs. Finally, plot these results, with the cut-offs on the x-axis and the accuracy on the y-axis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ401RR_xD1v"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW6S4w-60J5K"
      },
      "source": [
        "Analyse these results. How does the cut-off influence the prediction? The `some_model.predict() `function uses a 0.5 cut-off. How did this cut-off perform in your calculations? What cut-off would you recommend based on your results?\n",
        "___\n",
        "\n",
        "6. Use the results of your continuous predictions, extract the true and false positive rate and plot a ROC curve. Highlight the optimal cut-off based on task 5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiU8j5HQxQRN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqILyfIU1H5C"
      },
      "source": [
        "7. Manually define a cut-off in which the ratio of predictions of 0s (goods) and 1s (bads) is representative of the ratio of them in the training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0KHfEir1y9x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s2MnS5I1zQc"
      },
      "source": [
        "8. Assess your classifier by creating a precision-recall plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zoNw3SZ2Dq5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg3hHtVY2D3x"
      },
      "source": [
        "How does this table differ from the ROC curve? Which of them would you use in different situations?\n",
        "___\n",
        "\n",
        "Next we want to find out how the size of the training and test set can affect our predictions.\n",
        "9. Create a loop for the logit model. Vary the `test_size` parameter from 0 to 1 in steps of 0.1. Build the model on each training set, calculate the AUC values on the test set and save them, as well as its corresponding `test_size` parameter. Finally, plot the `test_size` on the x-axis and its corresponding AUC value on the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRX4x97_7nd2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWgNdd2M7oD1"
      },
      "source": [
        "Analyze the table to draw conclusions on how the test_size parameter influences the predictions, as well as the model itself? \n",
        "___\n",
        "\n",
        "To finish of on a high, we will finish with a less complex task.\n",
        "10. Familiarize yourself with the `StratifiedKFold()` function. We want to use this function to create 5 splits. Use the  ` cross_validate()` function to calculate the corresponding average AUC. Compare the results to the ones in task 4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwMDSPgF8E6-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DOXy8gP8GSo"
      },
      "source": [
        "Which model performed better? How do you explain these results? Read up on this function. How does it work and why would you use it?\n",
        "___\n",
        "___\n",
        "\n",
        "Well done!! You have reached the end of this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJzadg5GnYPv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}