{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "- The lecture slides illustrate the functioning of kMeans using pseudo-code. In [Tutorial](https://github.com/Humboldt-WI/bads/blob/master/tutorials/2_nb_descriptive_analytics.ipynb), we provide a *from scratch implementation* of the algorithm. Normally, the purpose of pseudo-code is to depict the main steps in an algorithm. Pseudo-code can then serve as a blueprint to implement the algorithm. Your task is to revert the process. Start from our custom Python implementation of kMeans in [Tutorial](https://github.com/Humboldt-WI/bads/blob/master/tutorials/2_nb_descriptive_analytics.ipynb) and write a piece of Pseudo-code that summarizes the functioning of the algorithm. Your pseudo-code should make use of all the custom function (i.e., by using their names) that we implement in [Tutorial](https://github.com/Humboldt-WI/bads/blob/master/tutorials/2_nb_descriptive_analytics.ipynb). For example, we provide a custom function `label_cluster()` as part of our kMeans implementation. Your pseudo-code should include a call to that function.  \n",
    "- Implement kMeans using a different distance measure, say cosine similarity of city-block metric\n",
    "- use the sklearn function `make_classification` to generate a more challenging data set and apply kMeans to that data. You can use the `sklearn` implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "Now that we introduced Pandas, students you apply k-means from tutorial 2 to the HMEQ data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using inbuilt functions from R Homework 1\n",
    "The density of the normal distribution with expected value $\\mu$ and variance $\\sigma$ is given as\n",
    "$$f(x | \\mu ,\\sigma ^{2}) = {\\frac {1}{\\sqrt {2\\sigma ^{2}\\pi}}}e^{-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a nice plot of the bell shape that is so famous. We already made sure that some libraries that we will need are imported. First, define two variables that store the two parameters of the normal distribution; no need to spill out these parameters, right?. Next generate some values $x$. Say you want to plot the bell curve for $x \\in \\{-3, 3\\} $. Use the NumPy function `linespace()` for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each value of $x$, compute the probability that a normally distributed random variable would be arbitrarily close to that value. To calculate the probability density of the normal distribution, you can use the function `norm.pdf`, which is part of the stats models library, which we import below. So you can write something like ` stats.norm.pdf(...)` where ... stands for the arguments that the function requires. Make sure to store the results of the computation in a variable **nvValues**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to plot. Create a simple graph of **nvValues** against **x** `plot()`function. Let's say you want your line to be in red color. Use the help and web search to find out how to plot a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borrow from R Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model assessment\n",
    "- produce prob predictions, vary threshold, and investigate who accuracy varies\n",
    "- manual ROC\n",
    "- we criticized accuracy for being dependent on the cut-off. Say you want to use accuracy, for example, because your manager ask you to produce an accuracy figure. To do better than using the default cut-off 0.5, you decide to use a cut-off such that the fraction of bad:good in the test data equates to that ration for the training:data. Implement this approach and calc accuracy \n",
    "\n",
    "\n",
    "# Regularization and model selection\n",
    "- implement logit from scratch with L2 regularization (https://towardsdatascience.com/implement-logistic-regression-with-l2-regularization-from-scratch-in-python-20bd4ee88a59)\n",
    "- we showcase tuning in sklearn with one test set. Implement nested cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "1. implement a bagged logistic regression classifier from scratch. You can use the `sklearn` class `LogisticRegressionClassifier` for implementing the base model. The actual bagging step, however, should be implemented from scratch\n",
    "2. Theory predicts that bagging should work better for unstable base models like trees than for stable base models like logistic regression. Use for custom bagging algorithm to verify this assertion for the HMEQ loan data set. Specifically:\n",
    "  - chose a proper experimental design to compare models (split-sample or cross-validation)\n",
    "  - Train a bagging classifier using logistic regression as base model\n",
    "  - Train a second bagging classifier using a tree as base model\n",
    "  - Both bagging classifiers should use your custom bagging function from task 1.\n",
    "  - Compare the predictive performance of the bagging ensembles on the test data\n",
    "3. (optional) Further enhance the previous analysis of task 2. as follows\n",
    "  - Repeat the comparison of bagged logit versus bagged trees multiple times with different training and testing data sets.\n",
    "  - Depict the results (predictive performance) as a box-plot\n",
    "  - Next, try out different settings for the hyperparameter *ensemble size*. Produce a line plot of predictive performance versus ensemble size for bagged logit and bagged tree. \n",
    "4. Write a custom Python function that implements the *Adaboost* algorithm. Follow the pseudo-code of the algorithm, as shown in the lecture materials. Design your function such that it accepts a `sklearn` model object as argument and than runs Adaboost using the corresponding base classifier. \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (adams)",
   "language": "python",
   "name": "pycharm-feb95198"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
