{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka20bzp6PcNj"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorials/8_nb_ensemble_learning.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swSaDjWvNDnY"
   },
   "source": [
    "# Chapter 8 - Ensemble learning \n",
    "We covered classification and regression trees in [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorials/5_nb_supervised_learning.ipynb) on supervised learning. Today, we will look into ways for improving the predictive power of tree-based classifiers by *ensemble learning*. Recall that an ensemble is a **composition of multiple base models**. The base models are prediction models themselves. In a homogeneous ensemble, we produce the base models with the same supervised learning algorithm. This is what we will do in this tutorial. Our underlying learning algorithm will be a classification tree. \n",
    "\n",
    "The lecture introduced three popular homogeneous ensemble frameworks, bagging, random forest, and boosting. We also learned that the latter framework is often implemented using *Adaboost* or, more recently, *gradient boosting*. The goal of the tutorial is to demonstrate the functioning of these ensemble learning frameworks. To that end, we provide **from scratch implementations of multiple ensemble learners**. Another goal is to empower you to use ensemble algorithms in your work/studies. We pursue this goal by walking you through coding demos of **how to train, tune, and apply ensemble learning algorithms** using `sklearn` and other libraries. \n",
    "\n",
    "\n",
    "The outline of the tutorial is as follows:\n",
    "- Preliminaries\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "We begin as usual with importing our standard libraries and also our standard modeling data. Since we are also familiar data organization by now, we will also use `sklearn` functionality to partition our data into a training and a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4172, 18) (4172, 1) (1788, 18) (1788, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import standard Python libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Some configuration of the plots we will create later\n",
    "%matplotlib inline  \n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Load credit risk data in pre-processed format from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq_modeling.csv' \n",
    "df = pd.read_csv(data_url, index_col=\"index\")\n",
    "\n",
    "# Pretty printing\n",
    "from pprint import pprint\n",
    "\n",
    "# Extract target variable and feature matrix \n",
    "X = df.drop(['BAD'], axis=1) \n",
    "y = df[['BAD']]\n",
    "\n",
    "# Split data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=888)\n",
    "print(\"Remember the shape of our data: \")\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "Even more effective for predictions than decision trees, random forests (RF) are useful and powerful ensemble methods for classifying and regressing data. Essentially, a random set of the features are taken to build decision trees. Trees are also built using *bagged data* which are samples of the data taken with replacement. Many trees are grown (sometimes their maximum depth is also specified) and then all trees vote on the response. Each tree's vote generally has the same weight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest from scratch\n",
    "Though most of the the time, `sklearn`  can be used to implement RF, it is useful to take a look at the inner workings of the algorithm. Given that RF is an extension of the *bagging algorithm*, we will not look into bagging separately but cover bagging as part of RF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Individual Trees\n",
    "\n",
    "Again, a RF model is a composition of individual decision trees. So let's start with a recap how an  individual trees works for our data:\n",
    "\n",
    "We face a classification problem. Thus, the goal of growing a tree is to obtain leaf nodes in which the class distribution is pure. Many potential ways to split the data are calculated (eg. we are going to go through all unique values of each column and finding the midpoint between sequential values). For each potential split, we evaluate whether it contributes toward our goal of separating the classes. More specifically, we calculate the **impurity** of the parent node and comparing it with the sum of the impurity scores of the child nodes, which would result from the candidate split. We also weight the child nodes' impurity scores by the number of cases (data points) that enter the left and right child node, respectively. \n",
    "\n",
    "There are three major impurity functions: **entropy, gini, and misclassification error**. We use entropy in our example. The split which yields the lowest impurity is chosen and the process is repeated for the new nodes (this is recursion). \n",
    "\n",
    "The method of choosing the split that yields the lowest impurity is called the **greedy search method**. The following functions will help the decision tree implement greedy search tactics on the data. The algorithm stops either when purity in each node is reached or when it has reached a maximum depth (max amount of recursions we allow) specified in our function.\n",
    "\n",
    "Many functions for the tree are not found in Python packages and it is cleaner to write them out first then put them together in our main algorithm. Each function below does a specific action, which will be used in our final tree function at the end. Most functions are just copy & pasted from [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorials/5_nb_supervised_learning.ipynb). We will highlight changes as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6aD0OZmEe2ZV"
   },
   "outputs": [],
   "source": [
    "def check_purity(y):\n",
    "    \n",
    "    'checks if a leaf node is perfectly pure, in other words, if the leaf node contains only one class'\n",
    "    \n",
    "    unique_classes = np.unique(y) #count number of classes in section of data\n",
    "\n",
    "    if len(unique_classes) == 1: #check if the node is pure\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Z-Pk6tTTe2ZY"
   },
   "outputs": [],
   "source": [
    "def classify_data(y):\n",
    "    \n",
    "    'classifies data according to the majority class of each leaf'\n",
    "    \n",
    "    unique_classes, counts_unique_classes = np.unique(y, return_counts=True)\n",
    "    #returns classes and no. of obs per class\n",
    "\n",
    "    index = counts_unique_classes.argmax() #index of class with most obs\n",
    "    classification = unique_classes[index] #class chosen for classification which is class with most obs\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "C6kLufUxe2Zd"
   },
   "outputs": [],
   "source": [
    "def split_data(X, y, split_column, split_value):\n",
    "    \n",
    "    'splits data based on specific value, will yield both a split for the features X and target y'\n",
    "    \n",
    "    split_column_values = X[:, split_column]\n",
    "\n",
    "    X_below = X[split_column_values <= split_value] #partitions data according to split values from previous functions\n",
    "    X_above = X[split_column_values >  split_value]\n",
    "    \n",
    "    y_below = y[split_column_values <= split_value]\n",
    "    y_above = y[split_column_values >  split_value]\n",
    "    \n",
    "    return X_below, X_above, y_below, y_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Kc6VPgoZe2Zf"
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    \n",
    "    'calculates entropy for each partition of data'\n",
    "    \n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum() #probability of each class\n",
    "    entropy = sum(probabilities * -np.log2(probabilities)) #could replace with Gini impurity or misclassification\n",
    "     \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xnRBl4KUe2Zj"
   },
   "outputs": [],
   "source": [
    "def calculate_overall_entropy(y_below, y_above): \n",
    "    \n",
    "    'calculates the total entropy after each split'\n",
    "       \n",
    "    n = len(y_below) + len(y_above)\n",
    "    p_data_below = len(y_below) / n\n",
    "    p_data_above = len(y_above) / n\n",
    "\n",
    "    overall_entropy =  (p_data_below * calculate_entropy(y_below)\n",
    "                      + p_data_above * calculate_entropy(y_above))\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CE2mvNCDe2Zl"
   },
   "outputs": [],
   "source": [
    "def determine_best_split(X, y, potential_splits):\n",
    "    \n",
    "    'selects which split lowered entropy the most'\n",
    "    \n",
    "    overall_entropy = 9999 #set arbitrarily high, the function will loop over and replace this with lower impurity values\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            X_below, X_above, y_below, y_above = split_data(X, y, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = calculate_overall_entropy(y_below, y_above)\n",
    "            \n",
    "            #goes through each potential split and only updates if it lowers entropy\n",
    "\n",
    "            if current_overall_entropy <= overall_entropy: \n",
    "                overall_entropy = current_overall_entropy #updates only if lower entropy split found, in the ned this is greedy search\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "            \n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random subspace\n",
    "Recall a distinct feature of RF compared to bagging. The algorithm for growing base model trees in RF does not search for the next best split among all features. Instead, it first draws a random subset of features and then finds the next best split among this subset. This mechanism is called random subspace and was introduced in [Ho's 1998 paper](http://dx.doi.org/10.1109/34.709601). \n",
    "\n",
    "Random subspace does not make sense when growing an individual tree. This is why we did not cover the approach in  [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorials/5_nb_supervised_learning.ipynb). However, random subspace is a vital ingredient to RF and it is best implemented in function to get potential splits. Therefore, we adjust the implementation of `get_potential_splits()` compared to the [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorials/5_nb_supervised_learning.ipynb) version, to ready our code for RF. Note that our updated implementation does not use random subspace by default but can activate it easily by setting the corresponding argument. Thus, our implementation supports both, vanilla trees and trees with random subspace.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pB28rbBoe2Za"
   },
   "outputs": [],
   "source": [
    "def get_potential_splits(X, random_subspace = None):\n",
    "    \n",
    "    'first, takes every unique value of every feature in the feature space, then finds the midpoint between each value'\n",
    "    'modified to add random_subspace for random forest'\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = X.shape #don't need rows, we choose the column to split on\n",
    "    # only need second value of .shape which is columns\n",
    "    column_indices = list(range(n_columns))\n",
    "    \n",
    "    if random_subspace and random_subspace <= len(column_indices): #randomly chosen features\n",
    "        column_indices = random.sample(population=column_indices, k=random_subspace)\n",
    "    \n",
    "    for column_index in column_indices:\n",
    "        potential_splits[column_index] = [] \n",
    "        values = X[:, column_index] \n",
    "        unique_values = np.unique(values) #get all unique values in each column\n",
    "\n",
    "        for index in range(len(unique_values)): #all unique feature values\n",
    "            if index != 0: #skip first value, we need the difference between next values\n",
    "                current_value = unique_values[index]\n",
    "                previous_value = unique_values[index - 1] #find a value and the next smallest value\n",
    "                potential_split = (current_value + previous_value) / 2 #find difference between the two as a potential split\n",
    "                \n",
    "                #consider all values which lie between two values as a potential split\n",
    "                \n",
    "                potential_splits[column_index].append(potential_split)\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7-hpFq5e2Zn"
   },
   "source": [
    "The tree will now implement the helper functions and display the decision which yielded the best split if printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nLJLzoaCe2Zn"
   },
   "outputs": [],
   "source": [
    "def decision_tree_algorithm(X, y, counter=0, min_samples=2, max_depth=5, random_subspace = None): \n",
    "\n",
    "    'same function as in the Decision Tree notebook but now we add random_subspace argument'\n",
    "\n",
    "    # data preparation\n",
    "    if counter == 0: # counter tells us how deep the tree is, this is before the tree is initiated\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = X.columns\n",
    "        X = X.values #change all to NumPy array for faster calculations\n",
    "        y = y.values\n",
    "    else:\n",
    "        data = X #if we have started the tree, X should already be a NumPy array from the code above \n",
    "    \n",
    "    # base cases\n",
    "    if (check_purity(y)) or (len(X) < min_samples) or (counter == max_depth):\n",
    "        classification = classify_data(y)\n",
    "        \n",
    "        return classification\n",
    "    \n",
    "    # recursive part\n",
    "    else:    \n",
    "        counter += 1 #tells us how deep the tree is\n",
    "\n",
    "        # helper functions \n",
    "        potential_splits = get_potential_splits(X, random_subspace) #check for all possible splits ONLY using the random subspace and not all features!\n",
    "        best_split_column, best_split_value = determine_best_split(X, y, potential_splits) #select best split based on entropy\n",
    "        X_below, X_above, y_below, y_above = split_data(X, y, best_split_column, best_split_value) #execute best split\n",
    "        \n",
    "        # code to explain decisions made by tree to users\n",
    "        feature_name = COLUMN_HEADERS[best_split_column]\n",
    "        question = \"{} <= {}\".format(feature_name, best_split_value) #initiate explanation of split\n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        # pull answers from tree\n",
    "        yes_answer = decision_tree_algorithm(X_below, y_below, counter, min_samples, max_depth, random_subspace)\n",
    "        no_answer = decision_tree_algorithm(X_above, y_above, counter, min_samples, max_depth, random_subspace)\n",
    "        \n",
    "        # ensure explanation actually shows useful information\n",
    "        if yes_answer == no_answer: #if decisions are the same, only display one\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for the Random Forest Algorithm\n",
    "Now that we have the infrastructure to build one individual classification tree, we can make use of it to grow a full forest of trees. However, the **strength-diversity trade-off** suggests that a naive combination of base models will not give a powerful ensemble model. Rather, for an ensemble learner to work well, we need base models that display both, strength and diversity. RF fosters diversity among its base models by **randomizing tree growing**. So we grow a forest of random trees. Specifically, the two mechanisms of randomization in RF are **bootstrap sampling** and **random subspace**. To implement the corresponding functionality, we will us the `random` library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #needed for bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this library, we can implement our bootstrap sampling mechanism as follows. Remember that a bootstrap sample is nothing but a random sample of the same size as the original data drawn *with replacement*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TppSOUVe2Zq"
   },
   "outputs": [],
   "source": [
    "def bootstrapping(X, y, n_bootstrap):\n",
    "\n",
    "  'takes a random set of observations of the size n_bootstrap'\n",
    "\n",
    "    bootstrap_indices = np.random.randint(low=0, high=len(X), size=n_bootstrap) #chooses random indices for the sample\n",
    "    X_bootstrapped = X.iloc[bootstrap_indices]\n",
    "    y_bootstrapped = y.iloc[bootstrap_indices]\n",
    "    \n",
    "    return X_bootstrapped, y_bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all this infrastructure in place, the actual implementation of the RF algorithm is straightforward. All we need to do is to grow as many base model trees as we like, and make sure we activate **random subspace** when growing the individual member trees from a **bootstrap sample** of the training data. Next to the number of trees and number of features to consider for random subspace, our implementation allows the user to specify the size of the bootstrap sample, which is useful to accelerate the algorithm, and the maximum depth of the tree models. That is quite some flexibility, although professional implementations of the RF algorithm offer many more parameters.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJp8a_tpe2Zu"
   },
   "outputs": [],
   "source": [
    "def random_forest_algorithm(X, y, n_trees, n_bootstrap, n_features, dt_max_depth):\n",
    "  'puts the bootstrap sample in the decision tree algorithm with max depth and the random subset of features set, in otherwords, builds the forest tree by tree'\n",
    "\n",
    "    forest = []\n",
    "    for i in range(n_trees): #loops for the amount of trees set to be in the forest\n",
    "        X_bootstrapped, y_bootstrapped = bootstrapping(X, y, n_bootstrap)\n",
    "        tree = decision_tree_algorithm(X_bootstrapped, y_bootstrapped, max_depth=dt_max_depth, random_subspace=n_features) #creates individual trees\n",
    "        forest.append(tree)\n",
    "    \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being almost ready to develop an  RF classifier, the only missing bit is some functionality to apply the trained model to new data. We split this functionality into two functions, one for predicting one example with one tree, and a wrapper functions that uses our `predict_example` function and calls is for all base models of our RF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NCdqwHoe2Zw"
   },
   "outputs": [],
   "source": [
    "def predict_example(example, tree, counter=0):\n",
    "\n",
    "  'takes one observation and predicts its class'\n",
    "    \n",
    "    if counter == 0 and isinstance(tree, dict) == False: # very shallow tree settings may only vote one way, this first if-statement takes its vote into account\n",
    "        return tree\n",
    "    \n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "    # ask question\n",
    "    if comparison_operator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    \n",
    "    # feature is categorical\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict_example(example, residual_tree)\n",
    "\n",
    "    \n",
    "# Gathers all test data\n",
    "def decision_tree_predictions(test_df, tree):\n",
    "  \n",
    "  'applies predict_example to all of the test set'\n",
    "\n",
    "    predictions = test_df.apply(predict_example, args=(tree,), axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3yXFPG8e2Zz"
   },
   "outputs": [],
   "source": [
    "def random_forest_predictions(test_df, forest):\n",
    "    df_predictions = {}\n",
    "    for i in range(len(forest)):\n",
    "        column_name = \"tree_{}\".format(i) #key for dictionary\n",
    "        predictions = decision_tree_predictions(test_df, tree=forest[i]) #predictions from trees\n",
    "        df_predictions[column_name] = predictions #insert predictions into dictionary\n",
    "\n",
    "    df_predictions = pd.DataFrame(df_predictions) #change dictionary to pandas DF\n",
    "    random_forest_predictions = df_predictions.mode(axis=1)[0] #take mode of predictions over trees for final prediction\n",
    "    #if there is an even number of predictions, just default to the first value (very unlikely with many trees)\n",
    "    \n",
    "    return random_forest_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mot6Okr5e2Z1"
   },
   "source": [
    "### Ready, stready, go...\n",
    "Well done, we have our custom-code RF classifier up and ready. Time to put it into action and produce some credit default risk predictions. Since our implementation is maybe not the most efficient one, let's stick to shallow trees and also set the other hyperparameters such that run times are bearable.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Rw92_Ssqe2Z2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_forest_algorithm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-144a64b0b791>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Grow RF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mforest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_forest_algorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trees\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_bootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_max_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'random_forest_algorithm' is not defined"
     ]
    }
   ],
   "source": [
    "# Grow RF \n",
    "forest = random_forest_algorithm(X, y, n_trees=50, n_bootstrap=800, n_features=8, dt_max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "LmEZQNMse2Z6",
    "outputId": "971d0fde-f498-414f-c840-e15037426d8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOAN <= -1.081076960402989': [{'MORTDUE <= -0.9988023477260403': [{'JOB_Self <= 0.5': [True,\n",
       "      {'CLAGE <= 1.319329723705905': [False, True]}]},\n",
       "    {'DEBTINC <= -0.36610310502357446': [{'JOB_Sales <= 0.5': [False, True]},\n",
       "      {'YOJ <= -0.5961885016974147': [{'VALUE <= -1.2866632148005883': [True,\n",
       "          False]},\n",
       "        {'CLNO <= -0.43208664907294947': [True, False]}]}]}]},\n",
       "  {'NINQ <= 0.3158144850582817': [{'DEBTINC <= 1.9169228230498017': [False,\n",
       "      True]},\n",
       "    {'DELINQcat_1+ <= 0.5': [{'CLAGE <= 0.0485464889016222': [{'CLAGE <= -1.4443818774196413': [True,\n",
       "          False]},\n",
       "        False]},\n",
       "      {'CLNO <= 0.5955631974189916': [{'CLNO <= -0.05347881089170803': [True,\n",
       "          False]},\n",
       "        True]}]}]}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depict one member tree as rule set\n",
    "forest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRV04WBke2Z9"
   },
   "outputs": [],
   "source": [
    "# Calculate predictions\n",
    "predictions = random_forest_predictions(X, forest) #get predictions from forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "upoB6SxNe2Z_",
    "outputId": "a8d8be81-1429-43cf-d3ff-dd6264181ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8701342281879194"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = np.mean(np.vstack(predictions) == np.array(y)) #check error rate\n",
    "\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfeanJuve2aB"
   },
   "source": [
    "The tree is able to classify a good proportion of the test set well even with only a depth of 3. \n",
    "\n",
    "Before moving on with using RF in `sklearn`, here is a **little exercise** for you:\n",
    "Use your customer implementation of RF to develop a bagging ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yourt task: use the above functions to train a bagging ensemble using decision tress as base model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest in sklearn\n",
    "As promised, we will also demonstrate how you would use RF in production environments, that is with some proper implementation under the hood. Luckily, RF is maybe one of the most widely available data science algorithm. You find professional implementations on almost every platform. Obviously, this also includes `sklearn`. \n",
    "\n",
    "It turns out that training and using RF in `sklearn`is super easy. Check out this code, which shows all it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\adams\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99, 0.01],\n",
       "       [0.48, 0.52],\n",
       "       [0.58, 0.42],\n",
       "       ...,\n",
       "       [0.58, 0.42],\n",
       "       [0.87, 0.13],\n",
       "       [0.89, 0.11]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # import library\n",
    "rf = RandomForestClassifier()                        # create model object\n",
    "rf.fit(X_train, y_train)                             # fit model to training set \n",
    "yhat = rf.predict_proba(X_test)                      # obtain test set predictions                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that was it. We promised it is easy, die we not ;)\n",
    "\n",
    "However, the above demo is simplistic and does not really illustrate how you would use RF in practice. That is mainly because we omit hyperparameters and their tuning. **Using an analytical model with some default parameters is never a good idea.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning RF hyperparameters using grid search\n",
    "Random forest is often considered robust toward hyperparameters settings. Still, some tuning may be beneficial. For model selection, we draw on the demos of [Tutorial 7](https://github.com/Humboldt-WI/bads/blob/master/tutorials/7_nb_model_selection.ipynb) and adjust these for our focal task of tuning RF. \n",
    "\n",
    "We consider the following hyperparameters. \n",
    "<br>\n",
    "- n_estimators: number of trees(models) in forest (ensemble)<br>\n",
    "- max_features : maximum features in random subspace<br>\n",
    "\n",
    "There are a couple of more hyperparameters. Normally, you would not need to tune them but for the sake of completeness, here are some more hyperparameters:\n",
    "- min_samples_split: minimum number of samples required in leaf node before another split is made. If it is less, this node won't split.<br>\n",
    "- min_samples_leaf: minimum number of samples required to be at a leaf node.<br>\n",
    "- max_leaf_nodes: maximum number of leaf nodes in a tree<br>\n",
    "- criterion: splitting function to use, e.g. gini coefficient<br>\n",
    "- max_depth: pruning parameter, maximum depth of decision tree<br>\n",
    "- n_jobs: parallelization of model building<br>\n",
    "- random_state: this parameter is used to define the random selection. It is used for comparison between various models.<br><br>\n",
    "\n",
    "If hyperparameters are not specified, they will be set to their default. \n",
    "\n",
    "**Remark**: Tuning RF might take a while. If you want to speed things up, consider setting the hyperparameter *max_samples*. It allows you to control the size of the bootstrap sample from which each tree is grown. Read the documentation for more information. Smaller sample sizes accelerate the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "print('Tuning random forest classifier')\n",
    "rf = RandomForestClassifier(random_state=888, max_samples = 0.5)  # This way, bootstrap sample size will be 50% of the training set\n",
    "\n",
    "# Define meta-parameter grid of candidate settings\n",
    "# The following settings are just for illustration\n",
    "param_grid = {'n_estimators': [100, 200, 500],\n",
    "              'max_features': [1, 2, 4]\n",
    "              }\n",
    "\n",
    "# Set up the grid object specifying the tuning options\n",
    "gs_rf = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc', verbose=1)\n",
    "gs_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we are interested in assessing our RF model in terms of ROC analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best CV AUC: %0.4f\" % gs_rf.best_score_)\n",
    "print(\"Optimal RF meta-parameters:\")\n",
    "print(gs_rf.best_params_)\n",
    "\n",
    "# Find test set auc of the best random forest classifier\n",
    "fp_rate, tp_rate, _ = metrics.roc_curve(y_test, gs_rf.predict_proba(X_test)[:, 1])\n",
    "auc_trace.update( {'rf' : metrics.auc(fp_rate, tp_rate)}) \n",
    "print('RF test set AUC: {:.4f}'.format(auc_trace['rf']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see some quite impressive AUC value. Let's plot the ROC curve to appreciate the power of our RF. This also shows how to access the final model from the grid-search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot is not new but note the use of gs_rf.best_estimator_ \n",
    "metrics.plot_roc_curve(gs_rf.best_estimator_, X_test, y_test)\n",
    "plt.plot([0, 1], [0, 1], \"r--\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "Boosting-type algorithms are based on **two core principles**\n",
    "1. Develop an ensemble sequentially (i.e., add one model at a time)\n",
    "2. Let the next model in the chain correct the errors of the current ensemble\n",
    "\n",
    "We discussed to flavors of boosting in the lecture, the **adaboost algorithm**, which was the first instantiation of a boosting ensemble, and **gradient boosting**, which was proposed in the seminal paper [Greedy Function Approximation: A Gradient Boosting Machine](https://www.jstor.org/stable/2699986?seq=1) by Jerome H. Friedman.\n",
    "\n",
    "This part of the tutorial follows the same approach as above, we first demonstrate implementing a boosting ensemble from scratch and then showcase its use together with a professional machine learning library. Many derivatives of boosting and libraries exist. Some members of the gradient boosting family include:\n",
    "- [LightGBM from Microsoft](https://lightgbm.readthedocs.io/en/latest/)\n",
    "- [Catboost from Yandex](https://catboost.ai/)\n",
    "- [NGBoost from Stanford ML Group](https://stanfordmlgroup.github.io/projects/ngboost/)\n",
    "\n",
    "Given wide adoption in practice and academia, we opted for focusing the tutorial to **extreme gradient boosting** or XGB for short. Demos for other popular boosting algorithms are available via the above links. Boosting algorithms that do not follow the gradient boosting principle may be considered somewhat out-of-fashion. We will not cover them here. However, the exercises give you an opportunity to implement your own Adaboost ensemble from scratch. Needless to say, `sklearn` also supports adaboost and other boosting algorithms.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31TaG5uJh0SS"
   },
   "source": [
    "## Verifying the boosting principle\n",
    "Is it true really true that one model can *learn* or *correct* the errors of another model as promised by the boosting paradigm? Before diving into cutting-edge gradient boosting, let's convince ourselves of this  fundamental premise of boosting. \n",
    "\n",
    "To that end, we use the HMEQ data set and demonstrate that training a model on the errors of a previous model helps to reduce classification error. Since it is common practice to implement boosting using trees as base model, we follow this approach.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V8lCORhgtWD4"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PFd6oVUtXNI"
   },
   "source": [
    "### Model training \n",
    "Here we will show the effectiveness of corrective models that *train on errors*. We will first train two models, the first will be for regular predictions. The second will predict which observations the first model misclassifies. We will first run the first prediction on test data, then correct these predictions using the second model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UZQCtRWzNDuE"
   },
   "outputs": [],
   "source": [
    "estimators = []  # list to store the two tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZYUxast5upkQ"
   },
   "outputs": [],
   "source": [
    "# train first classifier\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_split=2, max_depth=2) #first classifier\n",
    "dt = clf.fit(X_train, y_train) #fit the classifier\n",
    "estimators.append(('first model', dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having obtained our first model, we can calculate that model's residuals on the training set. Let's consider a classification setting. More concretely, let's focus on discrete class predictions. Recall that this is the type of output that we obtain when calling the function `predict()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w8SAYdlsutvz"
   },
   "outputs": [],
   "source": [
    "initial_pred = dt.predict(X_train)  # classify training set using first classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we identify misclassified observations and calculate the **classification error **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's produce a binary vector of the same size of the training set in which an entry of `True` indicates that the corresponding training set observation is misclassified by our first tree. The way we design our vector also makes it very easy to compute the classification error. All we need is to find the mean of that vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "IKHBSTiq7ADX",
    "outputId": "f4169b1d-652e-4f88-f666-9e7963e187c7"
   },
   "outputs": [],
   "source": [
    "res = initial_pred != y_train.iloc[:,0] \n",
    "print(\"Classification error of tree #1 is {:.4}\".format(res.mean()))\n",
    "print(\"Total number of errors tree #1 is {:.4}\".format(res.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to model #2. Here, we train not on $y$ but a new binary target variable indicating whether model #1 classified an observation correctly. We can think of this new target as $y - \\hat{y}$ aka a residual. However, since we train on decisions of a binary outcome, this classifier will predict errors of the first classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "x1SUtWIS9l5v",
    "outputId": "3f1ae2b4-8202-45cd-e27a-dac4e209b1ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = tree.DecisionTreeClassifier(criterion=\"gini\")  # second tree with different specs\n",
    "dt_res = clf2.fit(X_train, res)                       # note the new target\n",
    "estimators.append(('second model', dt_res))           # store second model for later\n",
    "dt_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `predict`function of our second corrective model, we obtain a prediction which observations model #1 is likely to misclassify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "RJX5qQTR_au6",
    "outputId": "9aa7ed17-db5c-4fdd-f969-18823281f522"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likely_misclassifications = dt_res.predict(X_train) \n",
    "print(\"Based on model #2, we expect model #1 to misclassify {} observations.\".format(\n",
    "    likely_misclassifications.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "lCBBxnW8Qx5_",
    "outputId": "45f1631d-f096-46ba-e873-6af169cea6c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "706     True\n",
       "3647    True\n",
       "4807    True\n",
       "1250    True\n",
       "3748    True\n",
       "        ... \n",
       "5218    True\n",
       "4060    True\n",
       "1346    True\n",
       "3454    True\n",
       "3582    True\n",
       "Name: BAD, Length: 4768, dtype: bool"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if classified likely misclassifications are the same as residuals\n",
    "accuracy_misclassifications = likely_misclassifications == res\n",
    "accuracy_misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qaTLynrfR2Bc",
    "outputId": "f349db1c-788f-462f-fbae-f3767eaa41e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It seems likely misclassifications are exactly congruent with residuals, so the model does work\n",
    "accuracy_misclassifications.mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZaQ61X4hJSS"
   },
   "source": [
    "### Model testing\n",
    "\n",
    "Now that we have our two models, we will begin using the test data to see if a combination of the two models can reduce the value of the residuals. We will first predict y using X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Fp-Wmz6sRGhv"
   },
   "outputs": [],
   "source": [
    "pred_initial_test = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "wdVk27zeRTh-",
    "outputId": "e88ff4a7-b512-4284-f13c-166312d909d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14093959731543623"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_test = pred_initial_test != y_test.iloc[:,0]\n",
    "print(\"Test error of model 1: {:.4}\".format(res_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM3lSiCrhUpY"
   },
   "source": [
    "Now we predict for which observations model 1 has likely made an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_BflWXFPRZxt",
    "outputId": "7e4fc3d5-f4c1-4b9e-989a-b719128047cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ...,  True, False, False])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likely_misclassifications_test = dt_res.predict(X_test)\n",
    "likely_misclassifications_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKWZfWWwhcWH"
   },
   "source": [
    "Lastly, we correct the likely misclassifications by simply flipping the predicted (from model 1) class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3cT6A821eUuA"
   },
   "outputs": [],
   "source": [
    "pred_corrected = pd.Series(pred_initial_test)\n",
    "pred_corrected[likely_misclassifications_test] = ~ pred_corrected[likely_misclassifications_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "YVipz0q5LcV6",
    "outputId": "a973143a-c87d-448a-be44-622f97a6ef7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True, False, False,  True,  True, False,\n",
       "        True,  True, False, False, False, False,  True, False,  True,\n",
       "        True,  True, False, False,  True, False,  True,  True, False,\n",
       "        True, False,  True, False,  True,  True,  True,  True, False,\n",
       "       False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True, False, False,\n",
       "       False,  True,  True, False, False, False,  True,  True,  True,\n",
       "        True, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True, False,  True, False,\n",
       "        True,  True,  True,  True, False, False, False, False,  True,\n",
       "        True,  True, False,  True, False,  True,  True,  True,  True,\n",
       "       False, False,  True, False,  True,  True, False, False, False,\n",
       "       False, False,  True,  True,  True, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True, False,\n",
       "        True,  True, False, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True, False,  True,  True])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_initial_test[likely_misclassifications_test]  # check that they have actually been changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "dGrQerZvfyTC",
    "outputId": "ac301bab-7050-4388-db42-06268f251e1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5        True\n",
       "13      False\n",
       "17       True\n",
       "20      False\n",
       "26       True\n",
       "        ...  \n",
       "1165     True\n",
       "1175    False\n",
       "1176     True\n",
       "1182    False\n",
       "1189    False\n",
       "Length: 159, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_corrected[likely_misclassifications_test]  # all the results are opposite, so this worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for the grand final, did we reduce the test error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "kI1XuYH8gCS0",
    "outputId": "b826edcd-019a-4339-9ee8-1ed4192b6a53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13003355704697986"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_corrected = np.array(pred_corrected) != y_test.iloc[:,0]\n",
    "print(\"Test error after corrected model 1 by model 2: {:.4}\".format(res_corrected.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBOLBv6YhnS_"
   },
   "source": [
    "Hurray!!!\n",
    "\n",
    "A lower test error indicates that our process worked. We were able to lower the error on a test set using a second model which focused on identifying misclassified cases. Let's now examine how gradient boosting relies on the same principled approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting from scratch\n",
    "Having convinced ourselves that the boosting idea works in principle, let's revisit the steps of Friedman's gradient boosting machine.\n",
    "\n",
    "Go through a from scratch implementation following one of the many examples like, e.g., \n",
    "\n",
    "- https://towardsdatascience.com/gradient-boosting-in-python-from-scratch-4a3d9077367\n",
    "- https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d\n",
    "- https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/gradient_boosting.py\n",
    "- https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/gradient_boosting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting in sklearn \n",
    "As said above, gradient boosting is not gradient boosting. Different gradient boosting algorithms exist and different implementations exist of each of those. If you want to use the *original gradient boosting machine* as proposed in [Friedman's 2001 paper](https://www.jstor.org/stable/2699986?seq=1) or his [follow-up paper on stochastic gradient boosting](http://dx.doi.org/10.1016/S0167-9473(01)00065-2), we recommend using the class `sklearn.ensemble.GradientBoostingClassifier`. It offers a lot of flexibility (e.g., hyperparameters) and is[ very well documented](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). However, for later data sets, you might want to use **extreme gradient boosting (XGB)**, which was explicitly designed for highly scalable gradient boosting. Below, we demonstrate application of XGB to our credit risk data set.\n",
    "\n",
    "Although the title of the chapter promises a `sklearn` implementation, it is actually common to use the `xgboost` library for training XGB models. To be precise, `sklearn` offers an implementation of XGB but this implementation might not incorporate the latest features. As far as the use is concerned, you will not notice any differences between the `xgboost` library and `sklearn`. However, **you might need to install the `xgboost` library before moving on**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning XGB hyperparameters\n",
    "Our modeling pipeline is exactly as before in the RF example, and in general. We tune hyperparameters using grid-search and cross-validating the training data. Once we determined good hyperparameter settings, we train a XGB model with the corresponding configuration on the entire training set and obtain test set prediction, which we assess using ROC analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "    # Setting up the grid of meta-parameters\n",
    "xgb_param_grid = {\n",
    "    'colsample_bytree': np.linspace(0.5, 0.9, 5),  # random subspace\n",
    "    'n_estimators': [100, 200],  # ensemble size or number of gradient steps\n",
    "    'max_depth': [5, 10],   # max depth of decision trees\n",
    "    'learning_rate': [0.1, 0.01],  # learning rate\n",
    "    'early_stopping_rounds': [10]}  # early stopping if no improvement after that many iterations\n",
    "\n",
    "gs_xgb = GridSearchCV(estimator=xgb.XGBClassifier(), param_grid=xgb_param_grid, scoring='roc_auc', cv=5, verbose=1)\n",
    "gs_xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best CV AUC: %0.4f\" % gs_xgb.best_score_)\n",
    "print(\"Optimal XGB meta-parameters:\")\n",
    "print(gs_xgb.best_params_)\n",
    "\n",
    "# Find test set auc of the best XGB  classifier\n",
    "fp_rate, tp_rate, _ = metrics.roc_curve(y_test, gs_xgb.predict_proba(x_test)[:, 1])\n",
    "print('XGB test set AUC with optimal meta-parameters: {:.4f}'.format( metrics.auc(fp_rate, tp_rate) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot is not new but note the use of gs_rf.best_estimator_ \n",
    "metrics.plot_roc_curve(gs_rf.best_estimator_, x_test, y_test)\n",
    "plt.plot([0, 1], [0, 1], \"r--\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGB models completes our journey through the space of ensemble learning algorithms. We have now covered the maybe most important of-the-shelf machine learning algorithms and you have seen how we can apply them to real-world data. \n",
    "\n",
    "In future tutorials, we will go back to other steps in the machine learning pipeline and learn about yet more possibilities and procedures to derive value from data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (adams)",
   "language": "python",
   "name": "pycharm-feb95198"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
