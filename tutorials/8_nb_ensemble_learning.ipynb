{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka20bzp6PcNj"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorials/8_nb_ensemble_learning.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swSaDjWvNDnY"
   },
   "source": [
    "# Chapter 8 - Ensemble learning \n",
    "We covered classification and regression trees in [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorials/5_nb_supervised_learning.ipynb) on supervised learning. Today, we will look into ways for improving the predictive power of tree-based classifiers by *ensemble learning*. Recall that an ensemble is a **composition of multiple base models**. The base models are prediction models themselves. In a homogeneous ensemble, we produce the base models with the same supervised learning algorithm. This is what we will do in this tutorial. Our underlying learning algorithm will be a classification tree. \n",
    "\n",
    "The lecture introduced three popular homogeneous ensemble frameworks, bagging, random forest, and boosting. We also learned that the latter framework is often implemented using *Adaboost* or, more recently, *gradient boosting*. The goal of the tutorial is to demonstrate the functioning of these ensemble learning frameworks. To that end, we provide **from scratch implementations of multiple ensemble learners**. Another goal is to empower you to use ensemble algorithms in your work/studies. We pursue this goal by walking you through coding demos of **how to train, tune, and apply ensemble learning algorithms** using `sklearn` and other libraries. \n",
    "\n",
    "\n",
    "The outline of the tutorial is as follows:\n",
    "- Preliminaries\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "We begin as usual with importing our standard libraries and also our standard modeling data. Since we are also familiar data organization by now, we will also use `sklearn` functionality to partition our data into a training and a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard Python libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Some configuration of the plots we will create later\n",
    "%matplotlib inline  \n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Load credit risk data in pre-processed format from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/Humboldt-WI/bads/master/data/hmeq_modeling.csv' \n",
    "df = pd.read_csv(data_url, index_col=\"index\")\n",
    "\n",
    "# Extract target variable and feature matrix \n",
    "X = df.drop(['BAD'], axis=1) \n",
    "y = df[['BAD']]\n",
    "\n",
    "# Split data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "Even more effective for predictions than decision trees, random forests (RF) are useful and powerful ensemble methods for classifying and regressing data. Essentially, a random set of the features are taken to build decision trees. Trees are also built using *bagged data* which are samples of the data taken with replacement. Many trees are grown (sometimes their maximum depth is also specified) and then all trees vote on the response. Each tree's vote generally has the same weight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest from scratch\n",
    "Though most of the the time, `sklearn`  can be used to implement RF, it is useful to take a look at the inner workings of the algorithm. Given that RF is an extension of the *bagging algorithm*, we will not look into bagging separately but cover bagging as part of RF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Individual Trees\n",
    "\n",
    "Again, a RF model is a composition of individual decision trees. So let's start with a recap how an  individual trees works for our data:\n",
    "\n",
    "We face a classification problem. Thus, the goal of growing a tree is to obtain leaf nodes in which the class distribution is pure. Many potential ways to split the data are calculated (eg. we are going to go through all unique values of each column and finding the midpoint between sequential values). For each potential split, we evaluate whether it contributes toward our goal of separating the classes. More specifically, we calculate the **impurity** of the parent node and comparing it with the sum of the impurity scores of the child nodes, which would result from the candidate split. We also weight the child nodes' impurity scores by the number of cases (data points) that enter the left and right child node, respectively. \n",
    "\n",
    "There are three major impurity functions: **entropy, gini, and misclassification error**. We use entropy in our example. The split which yields the lowest impurity is chosen and the process is repeated for the new nodes (this is recursion). \n",
    "\n",
    "The method of choosing the split that yields the lowest impurity is called the **greedy search method**. The following functions will help the decision tree implement greedy search tactics on the data. The algorithm stops either when purity in each node is reached or when it has reached a maximum depth (max amount of recursions we allow) specified in our function.\n",
    "\n",
    "# TODO @ADAM is that correct, all copy&past from 5 but get_potential split?\n",
    "Many functions for the tree are not found in Python packages and it is cleaner to write them out first then put them together in our main algorithm. Each function below does a specific action which will be used in our final tree function at the end. Most functions are just copy & pasted from [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorials/5_nb_supervised_learning.ipynb). We will highlight changes as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6aD0OZmEe2ZV"
   },
   "outputs": [],
   "source": [
    "def check_purity(y):\n",
    "    \n",
    "    'checks if a leaf node is perfectly pure, in other words, if the leaf node contains only one class'\n",
    "    \n",
    "    unique_classes = np.unique(y) #count number of classes in section of data\n",
    "\n",
    "    if len(unique_classes) == 1: #check if the node is pure\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Z-Pk6tTTe2ZY"
   },
   "outputs": [],
   "source": [
    "def classify_data(y):\n",
    "    \n",
    "    'classifies data according to the majority class of each leaf'\n",
    "    \n",
    "    unique_classes, counts_unique_classes = np.unique(y, return_counts=True)\n",
    "    #returns classes and no. of obs per class\n",
    "\n",
    "    index = counts_unique_classes.argmax() #index of class with most obs\n",
    "    classification = unique_classes[index] #class chosen for classification which is class with most obs\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "C6kLufUxe2Zd"
   },
   "outputs": [],
   "source": [
    "def split_data(X, y, split_column, split_value):\n",
    "    \n",
    "    'splits data based on specific value, will yield both a split for the features X and target y'\n",
    "    \n",
    "    split_column_values = X[:, split_column]\n",
    "\n",
    "    X_below = X[split_column_values <= split_value] #partitions data according to split values from previous functions\n",
    "    X_above = X[split_column_values >  split_value]\n",
    "    \n",
    "    y_below = y[split_column_values <= split_value]\n",
    "    y_above = y[split_column_values >  split_value]\n",
    "    \n",
    "    return X_below, X_above, y_below, y_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Kc6VPgoZe2Zf"
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    \n",
    "    'calculates entropy for each partition of data'\n",
    "    \n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum() #probability of each class\n",
    "    entropy = sum(probabilities * -np.log2(probabilities)) #could replace with Gini impurity or misclassification\n",
    "     \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xnRBl4KUe2Zj"
   },
   "outputs": [],
   "source": [
    "def calculate_overall_entropy(y_below, y_above): \n",
    "    \n",
    "    'calculates the total entropy after each split'\n",
    "       \n",
    "    n = len(y_below) + len(y_above)\n",
    "    p_data_below = len(y_below) / n\n",
    "    p_data_above = len(y_above) / n\n",
    "\n",
    "    overall_entropy =  (p_data_below * calculate_entropy(y_below)\n",
    "                      + p_data_above * calculate_entropy(y_above))\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CE2mvNCDe2Zl"
   },
   "outputs": [],
   "source": [
    "def determine_best_split(X, y, potential_splits):\n",
    "    \n",
    "    'selects which split lowered entropy the most'\n",
    "    \n",
    "    overall_entropy = 9999 #set arbitrarily high, the function will loop over and replace this with lower impurity values\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            X_below, X_above, y_below, y_above = split_data(X, y, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = calculate_overall_entropy(y_below, y_above)\n",
    "            \n",
    "            #goes through each potential split and only updates if it lowers entropy\n",
    "\n",
    "            if current_overall_entropy <= overall_entropy: \n",
    "                overall_entropy = current_overall_entropy #updates only if lower entropy split found, in the ned this is greedy search\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "            \n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random subspace\n",
    "Recall a distinct feature of RF compared to bagging. The algorithm for growing base model trees in RF does not search for the next best split among all features. Instead, it first draws a random subset of features and then finds the next best split among this subset. This mechanism is called random subspace and was introduced in [Ho's 1998 paper](http://dx.doi.org/10.1109/34.709601). \n",
    "\n",
    "Random subspace does not make sense when growing an individual tree. This is why we did not cover the approach in  [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorials/5_nb_supervised_learning.ipynb). However, random subspace is a vital ingredient to RF and it is best implemented in function to get potential splits. Therefore, we adjust the implementation of `get_potential_splits()` compared to the [Tutorial 5](https://github.com/Humboldt-WI/bads/blob/master/tutorials/5_nb_supervised_learning.ipynb) version, to ready our code for RF. Note that our updated implementation does not use random subspace by default but can activate it easily by setting the corresponding argument. Thus, our implementation supports both, vanilla trees and trees with random subspace.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pB28rbBoe2Za"
   },
   "outputs": [],
   "source": [
    "def get_potential_splits(X, random_subspace = None):\n",
    "    \n",
    "    'first, takes every unique value of every feature in the feature space, then finds the midpoint between each value'\n",
    "    'modified to add random_subspace for random forest'\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = X.shape #don't need rows, we choose the column to split on\n",
    "    # only need second value of .shape which is columns\n",
    "    column_indices = list(range(n_columns))\n",
    "    \n",
    "    if random_subspace and random_subspace <= len(column_indices): #randomly chosen features\n",
    "        column_indices = random.sample(population=column_indices, k=random_subspace)\n",
    "    \n",
    "    for column_index in column_indices:\n",
    "        potential_splits[column_index] = [] \n",
    "        values = X[:, column_index] \n",
    "        unique_values = np.unique(values) #get all unique values in each column\n",
    "\n",
    "        for index in range(len(unique_values)): #all unique feature values\n",
    "            if index != 0: #skip first value, we need the difference between next values\n",
    "                current_value = unique_values[index]\n",
    "                previous_value = unique_values[index - 1] #find a value and the next smallest value\n",
    "                potential_split = (current_value + previous_value) / 2 #find difference between the two as a potential split\n",
    "                \n",
    "                #consider all values which lie between two values as a potential split\n",
    "                \n",
    "                potential_splits[column_index].append(potential_split)\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7-hpFq5e2Zn"
   },
   "source": [
    "The tree will now implement the helper functions and display the decision which yielded the best split if printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nLJLzoaCe2Zn"
   },
   "outputs": [],
   "source": [
    "def decision_tree_algorithm(X, y, counter=0, min_samples=2, max_depth=5, random_subspace = None): \n",
    "\n",
    "    'same function as in the Decision Tree notebook but now we add random_subspace argument'\n",
    "\n",
    "    # data preparation\n",
    "    if counter == 0: # counter tells us how deep the tree is, this is before the tree is initiated\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = X.columns\n",
    "        X = X.values #change all to NumPy array for faster calculations\n",
    "        y = y.values\n",
    "    else:\n",
    "        data = X #if we have started the tree, X should already be a NumPy array from the code above \n",
    "    \n",
    "    # base cases\n",
    "    if (check_purity(y)) or (len(X) < min_samples) or (counter == max_depth):\n",
    "        classification = classify_data(y)\n",
    "        \n",
    "        return classification\n",
    "    \n",
    "    # recursive part\n",
    "    else:    \n",
    "        counter += 1 #tells us how deep the tree is\n",
    "\n",
    "        # helper functions \n",
    "        potential_splits = get_potential_splits(X, random_subspace) #check for all possible splits ONLY using the random subspace and not all features!\n",
    "        best_split_column, best_split_value = determine_best_split(X, y, potential_splits) #select best split based on entropy\n",
    "        X_below, X_above, y_below, y_above = split_data(X, y, best_split_column, best_split_value) #execute best split\n",
    "        \n",
    "        # code to explain decisions made by tree to users\n",
    "        feature_name = COLUMN_HEADERS[best_split_column]\n",
    "        question = \"{} <= {}\".format(feature_name, best_split_value) #initiate explanation of split\n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        # pull answers from tree\n",
    "        yes_answer = decision_tree_algorithm(X_below, y_below, counter, min_samples, max_depth, random_subspace)\n",
    "        no_answer = decision_tree_algorithm(X_above, y_above, counter, min_samples, max_depth, random_subspace)\n",
    "        \n",
    "        # ensure explanation actually shows useful information\n",
    "        if yes_answer == no_answer: #if decisions are the same, only display one\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for the Random Forest Algorithm\n",
    "Now that we have the infrastructure to build one individual classification tree, we can make use of it to grow a full forest of trees. However, the **strength-diversity trade-off** suggests that a naive combination of base models will not give a powerful ensemble model. Rather, for an ensemble learner to work well, we need base models that display both, strength and diversity. RF fosters diversity among its base models by **randomizing tree growing**. So we grow a forest of random trees. Specifically, the two mechanisms of randomization in RF are **bootstrap sampling** and **random subspace**. To implement the corresponding functionality, we will us the `random` library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #needed for bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this library, we can implement our bootstrap sampling mechanism as follows. Remember that a bootstrap sample is nothing but a random sample of the same size as the original data drawn *with replacement*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TppSOUVe2Zq"
   },
   "outputs": [],
   "source": [
    "def bootstrapping(X, y, n_bootstrap):\n",
    "\n",
    "  'takes a random set of observations of the size n_bootstrap'\n",
    "\n",
    "    bootstrap_indices = np.random.randint(low=0, high=len(X), size=n_bootstrap) #chooses random indices for the sample\n",
    "    X_bootstrapped = X.iloc[bootstrap_indices]\n",
    "    y_bootstrapped = y.iloc[bootstrap_indices]\n",
    "    \n",
    "    return X_bootstrapped, y_bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all this infrastructure in place, the actual implementation of the RF algorithm is straightforward. All we need to do is to grow as many base model trees as we like, and make sure we activate **random subspace** when growing the individual member trees from a **bootstrap sample** of the training data. Next to the number of trees and number of features to consider for random subspace, our implementation allows the user to specify the size of the bootstrap sample, which is useful to accelerate the algorithm, and the maximum depth of the tree models. That is quite some flexibility, although professional implementations of the RF algorithm offer many more parameters.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJp8a_tpe2Zu"
   },
   "outputs": [],
   "source": [
    "def random_forest_algorithm(X, y, n_trees, n_bootstrap, n_features, dt_max_depth):\n",
    "  'puts the bootstrap sample in the decision tree algorithm with max depth and the random subset of features set, in otherwords, builds the forest tree by tree'\n",
    "\n",
    "    forest = []\n",
    "    for i in range(n_trees): #loops for the amount of trees set to be in the forest\n",
    "        X_bootstrapped, y_bootstrapped = bootstrapping(X, y, n_bootstrap)\n",
    "        tree = decision_tree_algorithm(X_bootstrapped, y_bootstrapped, max_depth=dt_max_depth, random_subspace=n_features) #creates individual trees\n",
    "        forest.append(tree)\n",
    "    \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being almost ready to develop an  RF classifier, the only missing bit is some functionality to apply the trained model to new data. We split this functionality into two functions, one for predicting one example with one tree, and a wrapper functions that uses our `predict_example` function and calls is for all base models of our RF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NCdqwHoe2Zw"
   },
   "outputs": [],
   "source": [
    "def predict_example(example, tree, counter=0):\n",
    "\n",
    "  'takes one observation and predicts its class'\n",
    "    \n",
    "    if counter == 0 and isinstance(tree, dict) == False: # very shallow tree settings may only vote one way, this first if-statement takes its vote into account\n",
    "        return tree\n",
    "    \n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "    # ask question\n",
    "    if comparison_operator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    \n",
    "    # feature is categorical\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict_example(example, residual_tree)\n",
    "\n",
    "    \n",
    "# Gathers all test data\n",
    "def decision_tree_predictions(test_df, tree):\n",
    "  \n",
    "  'applies predict_example to all of the test set'\n",
    "\n",
    "    predictions = test_df.apply(predict_example, args=(tree,), axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3yXFPG8e2Zz"
   },
   "outputs": [],
   "source": [
    "def random_forest_predictions(test_df, forest):\n",
    "    df_predictions = {}\n",
    "    for i in range(len(forest)):\n",
    "        column_name = \"tree_{}\".format(i) #key for dictionary\n",
    "        predictions = decision_tree_predictions(test_df, tree=forest[i]) #predictions from trees\n",
    "        df_predictions[column_name] = predictions #insert predictions into dictionary\n",
    "\n",
    "    df_predictions = pd.DataFrame(df_predictions) #change dictionary to pandas DF\n",
    "    random_forest_predictions = df_predictions.mode(axis=1)[0] #take mode of predictions over trees for final prediction\n",
    "    #if there is an even number of predictions, just default to the first value (very unlikely with many trees)\n",
    "    \n",
    "    return random_forest_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mot6Okr5e2Z1"
   },
   "source": [
    "### Ready, stready, go...\n",
    "Well done, we have our custom-code RF classifier up and ready. Time to put it into action and produce some credit default risk predictions. Since our implementation is maybe not the most efficient one, let's stick to shallow trees and also set the other hyperparameters such that run times are bearable.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Rw92_Ssqe2Z2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_forest_algorithm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-144a64b0b791>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Grow RF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mforest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_forest_algorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trees\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_bootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_max_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'random_forest_algorithm' is not defined"
     ]
    }
   ],
   "source": [
    "# Grow RF \n",
    "forest = random_forest_algorithm(X, y, n_trees=50, n_bootstrap=800, n_features=8, dt_max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "LmEZQNMse2Z6",
    "outputId": "971d0fde-f498-414f-c840-e15037426d8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOAN <= -1.081076960402989': [{'MORTDUE <= -0.9988023477260403': [{'JOB_Self <= 0.5': [True,\n",
       "      {'CLAGE <= 1.319329723705905': [False, True]}]},\n",
       "    {'DEBTINC <= -0.36610310502357446': [{'JOB_Sales <= 0.5': [False, True]},\n",
       "      {'YOJ <= -0.5961885016974147': [{'VALUE <= -1.2866632148005883': [True,\n",
       "          False]},\n",
       "        {'CLNO <= -0.43208664907294947': [True, False]}]}]}]},\n",
       "  {'NINQ <= 0.3158144850582817': [{'DEBTINC <= 1.9169228230498017': [False,\n",
       "      True]},\n",
       "    {'DELINQcat_1+ <= 0.5': [{'CLAGE <= 0.0485464889016222': [{'CLAGE <= -1.4443818774196413': [True,\n",
       "          False]},\n",
       "        False]},\n",
       "      {'CLNO <= 0.5955631974189916': [{'CLNO <= -0.05347881089170803': [True,\n",
       "          False]},\n",
       "        True]}]}]}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depict one member tree as rule set\n",
    "forest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRV04WBke2Z9"
   },
   "outputs": [],
   "source": [
    "# Calculate predictions\n",
    "predictions = random_forest_predictions(X, forest) #get predictions from forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "upoB6SxNe2Z_",
    "outputId": "a8d8be81-1429-43cf-d3ff-dd6264181ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8701342281879194"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = np.mean(np.vstack(predictions) == np.array(y)) #check error rate\n",
    "\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfeanJuve2aB"
   },
   "source": [
    "The tree is able to classify a good proportion of the test set well even with only a depth of 3. \n",
    "\n",
    "Before moving on with using RF in `sklearn`, here is a **little exercise** for you:\n",
    "Use your customer implementation of RF to develop a bagging ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code of bagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest using sklearn\n",
    "As promised, we will also demonstrate how you would use RF in production environments, that is with some proper implementation under the hood. Luckily, RF is maybe one of the most widely available data science algorithm. You find professional implementations on almost every platform. Obviously, this also includes `sklearn`. \n",
    "\n",
    "It turns out that training and using RF in `sklearn`is super easy. Check out this code, which shows all it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\adams\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99, 0.01],\n",
       "       [0.48, 0.52],\n",
       "       [0.58, 0.42],\n",
       "       ...,\n",
       "       [0.58, 0.42],\n",
       "       [0.87, 0.13],\n",
       "       [0.89, 0.11]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # import library\n",
    "rf = RandomForestClassifier()                        # create model object\n",
    "rf.fit(X_train, y_train)                             # fit model to training set \n",
    "yhat = rf.predict_proba(X_test)                      # obtain test set predictions                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that was it. We promised it is easy, die we not ;)\n",
    "\n",
    "However, the above demo is simplistic and does not really illustrate how you would use RF in practice. That is mainly because we omit hyperparameters and their tuning. **Using an analytical model with some default parameters is never a good idea.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning RF hyperparameters using grid search\n",
    "Random forest is often considered robust toward hyperparameters settings. Still, some tuning may be beneficial. For model selection, we draw on the demos of [Tutorial 7](https://github.com/Humboldt-WI/bads/blob/master/tutorials/7_nb_model_selection.ipynb) and adjust these for our focal task of tuning RF. \n",
    "\n",
    "We consider the following hyperparameters. \n",
    "<br>\n",
    "- n_estimators: number of trees(models) in forest (ensemble)<br>\n",
    "- max_features : maximum features in random subspace<br>\n",
    "\n",
    "There are a couple of more hyperparameters. Normally, you would not need to tune them but for the sake of completeness, here are some more hyperparameters:\n",
    "- min_samples_split: minimum number of samples required in leaf node before another split is made. If it is less, this node won't split.<br>\n",
    "- min_samples_leaf: minimum number of samples required to be at a leaf node.<br>\n",
    "- max_leaf_nodes: maximum number of leaf nodes in a tree<br>\n",
    "- criterion: splitting function to use, e.g. gini coefficient<br>\n",
    "- max_depth: pruning parameter, maximum depth of decision tree<br>\n",
    "- n_jobs: parallelization of model building<br>\n",
    "- random_state: this parameter is used to define the random selection. It is used for comparison between various models.<br><br>\n",
    "\n",
    "If hyperparameters are not specified, they will be set to their default. \n",
    "\n",
    "**Remark**: Tuning RF might take a while. If you want to speed things up, consider setting the hyperparameter *max_samples*. It allows you to control the size of the bootstrap sample from which each tree is grown. Read the documentation for more information. Smaller sample sizes accelerate the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "print('Tuning random forest classifier')\n",
    "rf = RandomForestClassifier(random_state=888, max_samples = 0.5)  # This way, bootstrap sample size will be 50% of the training set\n",
    "\n",
    "# Define meta-parameter grid of candidate settings\n",
    "# The following settings are just for illustration\n",
    "param_grid = {'n_estimators': [100, 200, 500],\n",
    "              'max_features': [1, 2, 4]\n",
    "              }\n",
    "\n",
    "# Set up the grid object specifying the tuning options\n",
    "gs_rf = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc', verbose=1)\n",
    "gs_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we are interested in assessing our RF model in terms of ROC analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best CV AUC: %0.4f\" % gs_rf.best_score_)\n",
    "print(\"Optimal RF meta-parameters:\")\n",
    "print(gs_rf.best_params_)\n",
    "\n",
    "# Find test set auc of the best random forest classifier\n",
    "fp_rate, tp_rate, _ = metrics.roc_curve(y_test, gs_rf.predict_proba(X_test)[:, 1])\n",
    "auc_trace.update( {'rf' : metrics.auc(fp_rate, tp_rate)}) \n",
    "print('RF test set AUC: {:.4f}'.format(auc_trace['rf']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see some quite impressive AUC value. Let's plot the ROC curve to appreciate the power of our RF. This also shows how to access the final model from the grid-search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot is not new but note the use of gs_rf.best_estimator_ \n",
    "metrics.plot_roc_curve(gs_rf.best_estimator_, X_test, y_test)\n",
    "plt.plot([0, 1], [0, 1], \"r--\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (adams)",
   "language": "python",
   "name": "pycharm-feb95198"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
